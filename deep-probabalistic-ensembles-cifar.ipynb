{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active learning and deep probabalistic ensembles\n",
    "\n",
    "[Active learning](http://www.cs.northwestern.edu/~pardo/courses/mmml/papers/active_learning/improving_generalization_with_active_learning_ML94.pdf), loosely described, is an iterative process for getting the most out of your training data. This is especially useful for cases where you have a lot of unlabeled data that you would like to use for [supervised training](https://en.wikipedia.org/wiki/Supervised_learning) but labeling the data can is extremely time consuming and/or costly. In this case you want to be able choose which data points to label next strategically so that you total training set consists of a rich and diverse set of examples with minimal redundancy.\n",
    "\n",
    "Last week at PyData Miami, Cl√©ment Farabet of NVIDIA discussed how this problem relates to the massive amount of training data NVIDIA has collected for their autonomous car project. In particular he mentioned a paper that some of their researchers recently released that introduces \"[deep probabalistic ensembles](https://arxiv.org/pdf/1811.03575.pdf)\" which they apply alongside active learning as a solution to this problem. In this post we'll take a look at the ideas introduced in the paper.\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Why \"deep probabalistic ensembles\"? To answer this question, let's take a step back and consider a common pattern for active learning. Suppose we have two data sets at hand, $D_{L}=\\{X_{L}, y_{L}\\}$ and $D_{U}=\\{X_{U}\\}$, where the first data set is labeled and the second is not. Additionally suppose we have a model $f: X\\rightarrow y$ and a function $u: f(x)\\rightarrow \\mathbb{R}$ that measures the uncertainty of a given prediction $f(x)$. We can train $f$ on $D_{L}$ and exploit the uncertainty of the predictions $\\{f(x); x\\in X_{U}\\}$ to determine which data points in $X_{U}$ should be labeled next. For example, a sketch of this algorithm looks like\n",
    "\n",
    "1. Train $f$ on $D_{L}$.\n",
    "2. Obtain the predictions $\\{f(x); x\\in X_{U}\\}$.\n",
    "3. Use $u$ to calculate the uncertainty of each prediction.\n",
    "4. Select the top $n$ data points in $X_{U}$ where the model's predictions are most uncertain.\n",
    "\n",
    "After labeling these points we can update $D_{L}$ and repeat the process as desired.\n",
    "\n",
    "Of course, since we've mentioned the word \"uncertainty\" several times by now it should be clear why we are interested in deep probabalistic networks. Theoretically we can define a deep probabalistic, or bayesian, neural network as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "P(w \\ \\vert \\ x)=\\frac{P(w)P(w\\ \\vert \\ x)}{P(x)}\n",
    "\\label{eq:posterior}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $w$ is the set of all weights in the network for which we define the prior $P(w)$. From such a model we could obtain not only predictions of our target variable $y$ but also a measure of uncertainty for those predictions.\n",
    "\n",
    "However, it is well known that training a deep network like this is a difficult, if not impossible, task. Enter \"deep probabalistic ensembles\" which approximate of the posterior $P(w \\ \\vert \\ x)$.\n",
    "\n",
    "## Approximating deep bayesian neural networks\n",
    "\n",
    "Since NVIDIA paper is pretty short and self explanitory so I'll only cover the details necessary to grok the code below.\n",
    "\n",
    "The key is to train an ensemble of neural networks, of existing architectures such as [ResNet](https://arxiv.org/pdf/1512.03385.pdf), together with a single loss function that causes the ensemble to approximate samples from the posterior in \\eqref{eq:posterior}.\n",
    "\n",
    "To derive the objective, the authors begin by defining the usual objective in variational inference\n",
    "\n",
    "$$\n",
    "q^{*}=\\underset{q}{\\text{arg min}} \\mathbb{E}\\left [ \\text{log }{\\frac{q(w)}{p(w\\ \\vert \\ x)}} \\right]\n",
    "$$\n",
    "\n",
    "With some algebra they rewrite the objective as\n",
    "\n",
    "$$\n",
    "KL(q(w)\\vert\\vert p(w)) - \\mathbb{E}\\left[\\text{log }p(x \\ \\vert \\ w )\\right] + \\text{log }(p(x)\n",
    "$$\n",
    "\n",
    "To make the objective computationally tractable the last term, which is independent of $w$, is removed, resulting in a new objective, the [Evidence Lower Bound](https://en.wikipedia.org/wiki/Evidence_lower_bound),\n",
    "\n",
    "$$\n",
    "KL(q(w)\\vert\\vert p(w)) - \\mathbb{E}\\left[\\text{log }p(x \\ \\vert \\ w )\\right]\n",
    "$$\n",
    "\n",
    "During training, the second term in the equation above is approximated by aggregating the cross entropy for each model in the ensemble. The prior placed on the weights is specified as a guassian and the weights are completely pooled such that the first term can be calculated for a given layer (with some manipulation removing terms independent of $w$) as\n",
    "\n",
    "$$\n",
    "KL(q(w)\\vert\\vert p(w))=\\sum_{i}^{n_{i}n_{o}wh}{\\text{log }\\sigma_{i}^{2}+\\frac{2}{n_{o}wh\\sigma_{i}^{2}}+\\frac{\\mu_{i}^{2}}{\\sigma_{i}^{2}}}\n",
    "$$\n",
    "\n",
    "where $n_{i}$, $n_{o}$, $w$ and $h$ are the number of inputs and outputs and width and height of the kernel respectively. The means and variance are over the values of a particular weights accross the ensemble. Note deriving the equation above relies on centering the prior at 0 with variance $\\frac{2}{n_{o}wh}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras implementation\n",
    "\n",
    "Below is an implementation of the \"Deep probabalistic ensemble\" using `keras` and a simulation of the active learning experiments from the paper. Note that I was able to run the experiments on a [GTX 1060](https://www.nvidia.com/en-in/geforce/products/10series/geforce-gtx-1060/) but it was an overnight job.\n",
    "\n",
    "As in the NVIDIA paper, the ensemble consists of several ResNet18s. I used the [keras_contrib implementation](https://github.com/keras-team/keras-contrib/blob/d638cf409f7c8d3d042feac5269c12d507398eeb/keras_contrib/applications/resnet.py#L1) with just a few minor modifications so the returned models could be used as an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from resnet import ResNet\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def kl_regularization(layers):\n",
    "    \"\"\"Return the KL regularization penalty computed from a list of `layers`.\"\"\"\n",
    "    layers = K.stack(layers, axis=0)\n",
    "    layer_dims = K.cast_to_floatx(K.int_shape(layers[0]))\n",
    "    n_w = layer_dims[0]\n",
    "    n_h = layer_dims[1]\n",
    "    n_o = layer_dims[3]\n",
    "    mu_i = K.mean(layers, axis=0)\n",
    "    var_q_i = K.var(layers, axis=0)\n",
    "    var_p_i = 2 / (n_w * n_h * n_o)\n",
    "    kl_i = K.log(var_q_i) + (var_p_i / var_q_i) + (mu_i**2 / var_q_i)\n",
    "    return K.sum(kl_i)\n",
    "\n",
    "\n",
    "def ensemble_crossentropy(y_true, y_pred):\n",
    "    \"\"\"Return the cross entropy for the ensemble.\n",
    "    \n",
    "    Args:\n",
    "        y_true: Tensor with shape (None, batch_size, n_classes).\n",
    "        y_pred: Tensor with shape (None, batch_size, n_ensemble_members, n_classes).\n",
    "    \"\"\"\n",
    "    ensemble_entropy = K.categorical_crossentropy(y_true, y_pred, axis=-1)\n",
    "    return K.sum(ensemble_entropy, axis=-1)\n",
    "\n",
    "\n",
    "class Stack(keras.layers.Layer):\n",
    "    \"\"\"Subclass of a `keras.layers.Layer` that stacks outputs from several models\n",
    "    to create output for an ensemble.\n",
    "    \"\"\"\n",
    "    def call(self, X):\n",
    "        return K.stack(X, axis=1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # assumes all input shapes are the same\n",
    "        return (input_shape[0][0], len(input_shape), input_shape[0][1])\n",
    "\n",
    "\n",
    "class DeepProbabalisticEnsemble(keras.models.Model):\n",
    "    def __init__(self, input_shape, n_classes, n_members, beta=10**-5):\n",
    "        # instantiate the first member of the ensemble so we can reuse its input layer\n",
    "        # with the other layers\n",
    "        self.members = [ResNet(input_shape, classes=n_classes, block='basic', repetitions=[2, 2, 2, 2])]\n",
    "        self.members += [ResNet((32, 32, 3), classes=10, block='basic', input_layer=self.members[0].inputs[0],\n",
    "                                repetitions=[2, 2, 2, 2])\n",
    "                         for _ in range(n_members-1)]\n",
    "        outputs = Stack()([m.output for m in self.members])\n",
    "        self.beta = beta\n",
    "        super().__init__(inputs=self.members[0].inputs, outputs=outputs)\n",
    "    \n",
    "    @property\n",
    "    def losses(self):\n",
    "        \"\"\"Return all of the regularization penalties for the model.\n",
    "        \n",
    "        Overriding this property is the easiest way to have a loss that accounts for\n",
    "        weights in several layers at once in `keras`.\n",
    "        \"\"\"\n",
    "        losses = super().losses\n",
    "\n",
    "        # compute KL regularization\n",
    "        conv_layers = [\n",
    "            # kernel is index 0, bias is index 1\n",
    "            [L.trainable_weights[0] for L in m.layers if isinstance(L, keras.layers.Conv2D)]\n",
    "            for m in self.members]\n",
    "        # currently, each sublist is a list of model layers.\n",
    "        # realign these sublists to correspond to layers\n",
    "        conv_layers = [[L for L in layers] for layers in zip(*conv_layers)]\n",
    "        kl_regularizations = [self.beta * kl_regularization(layers) for layers in conv_layers]\n",
    "        losses.extend(kl_regularizations)\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpe = DeepProbabalisticEnsemble((32, 32, 3), 10, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "SVG(model_to_dot(dpe).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpe.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets.cifar10 import load_data\n",
    "import numpy as np\n",
    "from unittest import mock\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = (X_train - X_train.mean(axis=0)) / X_train.std(axis=0)\n",
    "X_test = (X_test - X_test.mean(axis=0)) / X_train.std(axis=0)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_train = y_train[:, np.newaxis, :]\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "y_test = y_test[:, np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import gc\n",
    "\n",
    "# set to 8 to replicate experiments in paper\n",
    "n_ensemble_members = 8\n",
    "# set to 4000 or 8000 to replicate experiments in paper\n",
    "budget = 4000\n",
    "# set to 8 to replicate experiments in paper\n",
    "n_iterations = 8\n",
    "# set to 400 to replicate experiments in paper\n",
    "max_epochs = 400\n",
    "# set to 25 to replicate experiments in paper\n",
    "patience = 25\n",
    "# keras param, if non-zero this can blow up the output in the cell below\n",
    "verbosity = 0\n",
    "\n",
    "# simulation\n",
    "dpe_builder = partial(DeepProbabalisticEnsemble, (32, 32, 3), 10, n_ensemble_members)\n",
    "b = budget // n_iterations\n",
    "n_acquisitions = b\n",
    "idx = np.random.choice(len(X_train), size=len(X_train), replace=False)\n",
    "X_labeled, y_labeled = X_train[idx[:b]], y_train[idx[:b]]\n",
    "X_unlabeled, y_unlabeled = X_train[idx[b:]], y_train[idx[b:]]\n",
    "while n_acquisitions < budget:\n",
    "    print('building ensemble')\n",
    "    dpe = dpe_builder()\n",
    "    dpe.compile(loss=ensemble_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    print('training ensemble')\n",
    "    history = dpe.fit(\n",
    "        X_labeled, y_labeled,\n",
    "        batch_size=32,\n",
    "        epochs=max_epochs,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[keras.callbacks.ReduceLROnPlateau(patience=25)],\n",
    "        verbose=verbosity)\n",
    "    print('validation loss:', history.history['val_loss'][-1],\n",
    "          'validation accuracy:', history.history['val_acc'][-1])\n",
    "\n",
    "    # aggregate along the individual model predictions\n",
    "    print('calculating uncertainty of predictions')\n",
    "    p = dpe.predict(X_unlabeled).sum(axis=1)\n",
    "    h_ens = (-p * np.log(p)).sum(axis=1)  # approximation of paper\n",
    "    idx_acquisitions = np.argsort(h_ens)[-b:]\n",
    "    idx_rest = np.argsort(h_ens)[:-b]\n",
    "    \n",
    "    print('adding %d examples to training data' % len(idx_acquisitions))\n",
    "    X_labeled = np.concatenate([X_labeled, X_unlabeled[idx_acquisitions]])\n",
    "    y_labeled = np.concatenate([y_labeled, y_unlabeled[idx_acquisitions]])\n",
    "    X_unlabeled = X_unlabeled[idx_rest]\n",
    "    y_unlabeled = y_unlabeled[idx_rest]\n",
    "    n_acquisitions += b\n",
    "    print('%d labeled examples' % len(X_labeled))\n",
    "    print('%d unlabeled examples' % len(X_unlabeled))\n",
    "    \n",
    "    print('releasing ensemble from GPU memory')\n",
    "    K.clear_session()\n",
    "    del dpe\n",
    "    del history\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
